{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Importance via Inner Loop Cross-Validation\n",
    "\n",
    "As machine learning practioners, we must always be wary of introducing bias into our algorithms, which can lead to overfitting. But this is rarely considered in the feature selection process, where it is commonplace to perform feature testing and selection prior to model fitting.\n",
    "\n",
    "However, according to Jason Brownlee, PhD [this is a mistake](https://machinelearningmastery.com/an-introduction-to-feature-selection/):\n",
    "\n",
    "> You must include feature selection within the inner-loop when you are using accuracy estimation methods such as cross-validation. This means that feature selection is performed on the prepared fold right before the model is trained. A mistake would be to perform feature selection first to prepare your data, then perform model selection and training on the selected features.\n",
    "\n",
    "The error occurs because our feature selection is *biased* to the training data. The model could then report inflated results upon testing. So rather than perform feature selection prior to model fitting, we should do it within cross-validated loops. This way, we select the features that perform best when applied to unseen data.\n",
    "\n",
    "So how do we summarize what happens when we do feature testing within each loop? Brownlee [suggests](https://machinelearningmastery.com/k-fold-cross-validation/#comment-450666):\n",
    "\n",
    ">you will get different features, and perhaps you can take the average across the findings from each fold.\n",
    "\n",
    "This suggestion will be implemented in examples in this notebook for XGBoost classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import KFold\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sepal Length</th>\n",
       "      <th>Sepal Width</th>\n",
       "      <th>Petal Length</th>\n",
       "      <th>Petal Width</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>6.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>6.2</td>\n",
       "      <td>3.4</td>\n",
       "      <td>5.4</td>\n",
       "      <td>2.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>5.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.1</td>\n",
       "      <td>1.8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Sepal Length  Sepal Width  Petal Length  Petal Width\n",
       "0             5.1          3.5           1.4          0.2\n",
       "1             4.9          3.0           1.4          0.2\n",
       "2             4.7          3.2           1.3          0.2\n",
       "147           6.5          3.0           5.2          2.0\n",
       "148           6.2          3.4           5.4          2.3\n",
       "149           5.9          3.0           5.1          1.8"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import Iris dataset\n",
    "iris = datasets.load_iris()\n",
    "y = iris.target\n",
    "print(y)\n",
    "# 0 is Setosa, 1 is Versicolour, and 2 is Virginica\n",
    "X = pd.DataFrame(iris.data, columns=['Sepal Length', 'Sepal Width', 'Petal Length', 'Petal Width'])\n",
    "# Show features\n",
    "X.iloc[[0, 1, 2, -3, -2, -1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Importance\n",
    "\n",
    "Most tree-based machine learning models contain feature importance methods, which estimate how relevant a feature is in predicting outcomes. We'll use the scores returned in each cross-validation fold. Ordinarily, we would split the dataset into train/validation/test sets. Since the purpose of this notebook is not to train and test models, we'll assume the entire Iris dataset is the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def xgb_cv_feature_importance(X, y, split, clf, eval_metric, esr=10):\n",
    "    '''\n",
    "    Example of XGBoost classification implementation of inner-loop cross-validation feature importance\n",
    "    Must be modified for other algorithms\n",
    "    \n",
    "    Returns the best selected features from X along with feature importances\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : {Pandas DataFrame}, shape = [n_samples, n_features]\n",
    "        Training vectors, where n_samples is the number of samples and\n",
    "        n_features is the number of features.\n",
    "    y : array-like, shape = [n_samples]\n",
    "        Target values.\n",
    "    split : generator object that splits the data\n",
    "    clf : XGBoost classifier object\n",
    "    eval_metric : string\n",
    "                  XGBoost classifier evaluation metric. \n",
    "    esr: int (default: 10)\n",
    "         Number of early stopping rounds to help prevent overfitting.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Pandas DataFrame of average feature importances, shape={n_samples, n_features}\n",
    "    '''\n",
    "    \n",
    "    # Create lists of selected features and values\n",
    "    selected_features = []\n",
    "    selected_vals = []\n",
    "    \n",
    "    count = 1\n",
    "    for train_index, test_index in split:\n",
    "        print(\"Split #\", count, \"\\nTRAIN:\", train_index, \"\\nTEST:\", test_index)\n",
    "        \n",
    "        # Use the splits to divide into X_train, y_train, X_test, y_test and fit\n",
    "        X_train, X_test = X.iloc[train_index].values, X.iloc[test_index].values\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        eval_set = [(X_train, y_train), (X_test, y_test)]\n",
    "        clf.fit(X_train, y_train, eval_metric=eval_metric, eval_set=eval_set, early_stopping_rounds=esr, verbose=False)\n",
    "        \n",
    "        # Add features and values to lists\n",
    "        feature_names = sorted([(X.columns[i], clf.feature_importances_[i]) for i in \n",
    "                                range(len(X.columns))], key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        selected_feature = [x[0] for x in feature_names]\n",
    "        selected_val = [x[1] for x in feature_names]\n",
    "        selected_features.extend(selected_feature)\n",
    "        selected_vals.extend(selected_val)\n",
    "        print(\"Sorted Feature Importance:\", feature_names)\n",
    "        print()\n",
    "        count += 1\n",
    "    \n",
    "    # Create Dataframe of average feature importances\n",
    "    selected_together = list(zip(selected_features, selected_vals))\n",
    "    selected_df = pd.DataFrame(selected_together, columns=['Selected_Cols', 'Importance'])\n",
    "    selected_df['Avg. Importance'] = selected_df.groupby('Selected_Cols')['Importance'].transform(lambda x: x.mean())\n",
    "    selected_df = selected_df.groupby('Selected_Cols').first().sort_values(by='Avg. Importance', \n",
    "                                                                           ascending=False).drop('Importance', 1)\n",
    "    return selected_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set remaining parameters\n",
    "kf = KFold(n_splits=3, shuffle=True)\n",
    "kf_split = kf.split(X, y)\n",
    "xgb = XGBClassifier(objective='binary:logistic', n_estimators=50)\n",
    "eval_metric = 'mlogloss'\n",
    "esr = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split # 1 \n",
      "TRAIN: [  0   1   2   4   6   8   9  13  14  16  17  18  19  20  22  24  25  26\n",
      "  27  30  31  32  33  35  36  37  39  41  42  43  44  46  49  51  52  53\n",
      "  54  55  57  59  61  63  65  66  67  68  70  73  74  75  76  77  78  79\n",
      "  80  82  83  85  87  90  91  93  94  95  96  97  98 100 102 103 104 106\n",
      " 110 111 112 115 116 118 119 120 121 122 123 124 125 127 131 133 134 135\n",
      " 137 138 139 140 142 143 144 145 148 149] \n",
      "TEST: [  3   5   7  10  11  12  15  21  23  28  29  34  38  40  45  47  48  50\n",
      "  56  58  60  62  64  69  71  72  81  84  86  88  89  92  99 101 105 107\n",
      " 108 109 113 114 117 126 128 129 130 132 136 141 146 147]\n",
      "Sorted Feature Importance: [('Petal Length', 0.5084746), ('Petal Width', 0.2857143), ('Sepal Length', 0.118644066), ('Sepal Width', 0.08716707)]\n",
      "\n",
      "Split # 2 \n",
      "TRAIN: [  2   3   5   6   7   8   9  10  11  12  14  15  18  20  21  22  23  24\n",
      "  25  28  29  30  31  34  35  36  38  39  40  41  43  45  46  47  48  49\n",
      "  50  51  53  56  58  59  60  62  64  65  66  67  69  70  71  72  76  77\n",
      "  78  80  81  82  84  86  87  88  89  92  94  96  99 100 101 102 103 105\n",
      " 106 107 108 109 111 113 114 117 118 120 121 126 128 129 130 131 132 133\n",
      " 134 136 137 138 140 141 143 144 146 147] \n",
      "TEST: [  0   1   4  13  16  17  19  26  27  32  33  37  42  44  52  54  55  57\n",
      "  61  63  68  73  74  75  79  83  85  90  91  93  95  97  98 104 110 112\n",
      " 115 116 119 122 123 124 125 127 135 139 142 145 148 149]\n",
      "Sorted Feature Importance: [('Petal Length', 0.5263158), ('Petal Width', 0.3), ('Sepal Width', 0.110526316), ('Sepal Length', 0.06315789)]\n",
      "\n",
      "Split # 3 \n",
      "TRAIN: [  0   1   3   4   5   7  10  11  12  13  15  16  17  19  21  23  26  27\n",
      "  28  29  32  33  34  37  38  40  42  44  45  47  48  50  52  54  55  56\n",
      "  57  58  60  61  62  63  64  68  69  71  72  73  74  75  79  81  83  84\n",
      "  85  86  88  89  90  91  92  93  95  97  98  99 101 104 105 107 108 109\n",
      " 110 112 113 114 115 116 117 119 122 123 124 125 126 127 128 129 130 132\n",
      " 135 136 139 141 142 145 146 147 148 149] \n",
      "TEST: [  2   6   8   9  14  18  20  22  24  25  30  31  35  36  39  41  43  46\n",
      "  49  51  53  59  65  66  67  70  76  77  78  80  82  87  94  96 100 102\n",
      " 103 106 111 118 120 121 131 133 134 137 138 140 143 144]\n",
      "Sorted Feature Importance: [('Petal Length', 0.618123), ('Petal Width', 0.26213592), ('Sepal Width', 0.106796116), ('Sepal Length', 0.012944984)]\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Avg. Importance</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Selected_Cols</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Petal Length</th>\n",
       "      <td>0.550971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Petal Width</th>\n",
       "      <td>0.282617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sepal Width</th>\n",
       "      <td>0.101497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sepal Length</th>\n",
       "      <td>0.064916</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Avg. Importance\n",
       "Selected_Cols                 \n",
       "Petal Length          0.550971\n",
       "Petal Width           0.282617\n",
       "Sepal Width           0.101497\n",
       "Sepal Length          0.064916"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_cv_feature_importance(X, y, kf_split, xgb, eval_metric, esr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More Complex Feature Importance\n",
    "\n",
    "The Iris dataset only has four features, so there is no real need to prune the features to the most important ones. More likely, we'll be using datasets with many more features. In such scenarios, the effect of features might be masked due to similarity with other features. One possible solution is to initially prune features based on importance, then run a second feature importance test. In the following example, we'll select the top 3 features to pass to the second importance test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def xgb_cv_feature_importance_complex(num_features, X, y, split, clf, eval_metric, esr=10):\n",
    "    '''\n",
    "    Example of XGBoost classification implementation of inner-loop cross-validation feature importance\n",
    "    Must be modified for other algorithms\n",
    "    \n",
    "    Returns the best selected features from X along with feature importances\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    num_features : int\n",
    "                   The number of features to pass to the second importance test. \n",
    "    X : {Pandas DataFrame}, shape = [n_samples, n_features]\n",
    "        Training vectors, where n_samples is the number of samples and\n",
    "        n_features is the number of features.\n",
    "    y : array-like, shape = [n_samples]\n",
    "        Target values.\n",
    "    split : generator object that splits the data\n",
    "    clf : XGBoost classifier object\n",
    "    eval_metric : string\n",
    "                  XGBoost classifier evaluation metric. \n",
    "    esr: int (default: 10)\n",
    "         Number of early stopping rounds to help prevent overfitting.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Pandas DataFrame of feature importances, shape={n_samples, n_features}\n",
    "    '''\n",
    "    \n",
    "    selected_features = []\n",
    "    selected_vals = []\n",
    "    count = 1\n",
    "    for train_index, test_index in split:\n",
    "        print(\"Split #\", count, \"\\nTRAIN:\", train_index, \"\\nTEST:\", test_index)\n",
    "        X_train, X_test = X.iloc[train_index].values, X.iloc[test_index].values\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        eval_set = [(X_train, y_train), (X_test, y_test)]\n",
    "        clf.fit(X_train, y_train, eval_metric=eval_metric, eval_set=eval_set, early_stopping_rounds=esr, verbose=False)\n",
    "        feature_names = sorted([(X.columns[i], clf.feature_importances_[i]) for i in \n",
    "                                range(len(X.columns))], key=lambda x: x[1], reverse=True)[:num_features]\n",
    "        \n",
    "        \n",
    "        selected = [x[0] for x in feature_names]\n",
    "        X_train, X_test = X.iloc[train_index][selected].values, X.iloc[test_index][selected].values\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        eval_set = [(X_train, y_train), (X_test, y_test)]\n",
    "        clf.fit(X_train, y_train, eval_metric=eval_metric, eval_set=eval_set, early_stopping_rounds=esr, verbose=False)\n",
    "        feature_names = sorted([(selected[i], clf.feature_importances_[i]) for i in \n",
    "                                range(len(selected))], key=lambda x: x[1], reverse=True)\n",
    "        selected_feature = [x[0] for x in feature_names]\n",
    "        selected_val = [x[1] for x in feature_names]\n",
    "        selected_features.extend(selected_feature)\n",
    "        selected_vals.extend(selected_val)\n",
    "        print(\"Sorted Feature Importance:\", feature_names)\n",
    "        print()\n",
    "        count += 1\n",
    "\n",
    "    selected_together = list(zip(selected_features, selected_vals))\n",
    "    selected_df = pd.DataFrame(selected_together, columns=['Selected_Cols', 'Importance'])\n",
    "    selected_df['# Selections'] = selected_df.groupby('Selected_Cols')['Selected_Cols'].transform('count')\n",
    "    selected_df['Avg. Importance'] = selected_df.groupby('Selected_Cols')['Importance'].transform(lambda x: x.sum()/(count-1))\n",
    "    selected_df = selected_df.groupby('Selected_Cols').first().sort_values(by='Avg. Importance', \n",
    "                                                                           ascending=False).drop('Importance', 1)\n",
    "    return selected_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KFold(n_splits=3, shuffle=True)\n",
    "kf_split = kf.split(X, y)\n",
    "xgb = XGBClassifier(objective='binary:logistic', n_estimators=50)\n",
    "eval_metric = 'mlogloss'\n",
    "esr = 10\n",
    "\n",
    "num_features = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split # 1 \n",
      "TRAIN: [  0   1   2   3   5   9  10  11  12  13  14  15  16  17  20  21  22  23\n",
      "  25  27  28  29  30  31  35  36  37  38  39  40  42  43  44  45  47  49\n",
      "  50  51  52  53  54  55  56  59  61  62  64  65  66  67  69  70  72  73\n",
      "  74  75  76  77  78  79  81  82  83  85  86  88  89  91  92  94  97 105\n",
      " 106 108 109 110 113 115 118 119 121 123 125 128 129 130 131 132 133 135\n",
      " 136 137 138 139 141 142 143 147 148 149] \n",
      "TEST: [  4   6   7   8  18  19  24  26  32  33  34  41  46  48  57  58  60  63\n",
      "  68  71  80  84  87  90  93  95  96  98  99 100 101 102 103 104 107 111\n",
      " 112 114 116 117 120 122 124 126 127 134 140 144 145 146]\n",
      "Sorted Feature Importance: [('Petal Length', 0.5075), ('Petal Width', 0.325), ('Sepal Width', 0.1675)]\n",
      "\n",
      "Split # 2 \n",
      "TRAIN: [  2   4   5   6   7   8  10  12  15  16  17  18  19  22  23  24  25  26\n",
      "  28  32  33  34  35  37  38  40  41  43  44  46  48  49  54  55  57  58\n",
      "  60  62  63  65  66  67  68  69  70  71  72  76  77  79  80  81  83  84\n",
      "  85  86  87  90  92  93  94  95  96  98  99 100 101 102 103 104 107 110\n",
      " 111 112 114 115 116 117 118 120 122 123 124 126 127 128 129 130 131 134\n",
      " 137 138 140 141 143 144 145 146 147 148] \n",
      "TEST: [  0   1   3   9  11  13  14  20  21  27  29  30  31  36  39  42  45  47\n",
      "  50  51  52  53  56  59  61  64  73  74  75  78  82  88  89  91  97 105\n",
      " 106 108 109 113 119 121 125 132 133 135 136 139 142 149]\n",
      "Sorted Feature Importance: [('Petal Length', 0.6023392), ('Petal Width', 0.25730994), ('Sepal Length', 0.14035088)]\n",
      "\n",
      "Split # 3 \n",
      "TRAIN: [  0   1   3   4   6   7   8   9  11  13  14  18  19  20  21  24  26  27\n",
      "  29  30  31  32  33  34  36  39  41  42  45  46  47  48  50  51  52  53\n",
      "  56  57  58  59  60  61  63  64  68  71  73  74  75  78  80  82  84  87\n",
      "  88  89  90  91  93  95  96  97  98  99 100 101 102 103 104 105 106 107\n",
      " 108 109 111 112 113 114 116 117 119 120 121 122 124 125 126 127 132 133\n",
      " 134 135 136 139 140 142 144 145 146 149] \n",
      "TEST: [  2   5  10  12  15  16  17  22  23  25  28  35  37  38  40  43  44  49\n",
      "  54  55  62  65  66  67  69  70  72  76  77  79  81  83  85  86  92  94\n",
      " 110 115 118 123 128 129 130 131 137 138 141 143 147 148]\n",
      "Sorted Feature Importance: [('Petal Length', 0.6409396), ('Petal Width', 0.2885906), ('Sepal Width', 0.0704698)]\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th># Selections</th>\n",
       "      <th>Avg. Importance</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Selected_Cols</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Petal Length</th>\n",
       "      <td>3</td>\n",
       "      <td>0.583593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Petal Width</th>\n",
       "      <td>3</td>\n",
       "      <td>0.290300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sepal Width</th>\n",
       "      <td>2</td>\n",
       "      <td>0.079323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sepal Length</th>\n",
       "      <td>1</td>\n",
       "      <td>0.046784</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               # Selections  Avg. Importance\n",
       "Selected_Cols                               \n",
       "Petal Length              3         0.583593\n",
       "Petal Width               3         0.290300\n",
       "Sepal Width               2         0.079323\n",
       "Sepal Length              1         0.046784"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_cv_feature_importance_complex(num_features, X, y, kf_split, xgb, eval_metric, esr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "With feature importances on unseen data in hand, we can more reliably select features that avoid pitfalls of bias that lead to overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
